{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644b1b83-fa70-4ae0-81d0-db27c5d2494b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. ê²½ë¡œ ì„¤ì • \n",
    "BASE_PATH = \"/home/ec2-user/.cache/kagglehub/datasets/Cornell-University/arxiv/versions/264\"\n",
    "INPUT_FILE = os.path.join(BASE_PATH, 'arxiv-metadata-oai-snapshot.json')\n",
    "OUTPUT_FILE = 'arxiv_cs_recent_filtered.csv'\n",
    "\n",
    "# í•„í„°ë§ ì¡°ê±´\n",
    "TARGET_CATEGORIES = {'cs.CV', 'cs.CL', 'cs.LG', 'cs.AI', 'cs.RO'} \n",
    "START_YEAR = 2020\n",
    "\n",
    "def parse_arxiv_data():\n",
    "    data = []\n",
    "    print(f\"ğŸ“‚ ì½ì–´ì˜¬ íŒŒì¼ ê²½ë¡œ: {INPUT_FILE}\")\n",
    "    print(\"ğŸš€ ë°ì´í„° í•„í„°ë§ ì‹œì‘ (CS ë¶„ì•¼ & 2020ë…„ ì´í›„)...\")\n",
    "    \n",
    "    # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"âŒ ì˜¤ë¥˜: í•´ë‹¹ ê²½ë¡œì— íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”: {INPUT_FILE}\")\n",
    "        return\n",
    "\n",
    "    with open(INPUT_FILE, 'r') as f:\n",
    "        for line in tqdm(f, desc=\"Processing\"):\n",
    "            try:\n",
    "                doc = json.loads(line)\n",
    "                \n",
    "                # 1. ì¹´í…Œê³ ë¦¬ í•„í„°ë§ (ì†ë„ ìµœì í™”ë¥¼ ìœ„í•´ ë¨¼ì € ì²´í¬)\n",
    "                if not 'cs.' in doc['categories']: \n",
    "                    continue\n",
    "                \n",
    "                cats = set(doc['categories'].split())\n",
    "                if not cats.intersection(TARGET_CATEGORIES):\n",
    "                    continue\n",
    "                \n",
    "                # 2. ë‚ ì§œ í•„í„°ë§\n",
    "                update_date = datetime.strptime(doc['update_date'], \"%Y-%m-%d\")\n",
    "                if update_date.year < START_YEAR:\n",
    "                    continue\n",
    "                \n",
    "                # 3. ë°ì´í„° í™•ë³´\n",
    "                data.append({\n",
    "                    'id': doc['id'],\n",
    "                    'title': doc['title'].replace('\\n', ' ').strip(),\n",
    "                    'abstract': doc['abstract'].replace('\\n', ' ').strip(),\n",
    "                    'categories': doc['categories'],\n",
    "                    'year': update_date.year,\n",
    "                    'date': doc['update_date']\n",
    "                })\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "    # CSV ì €ì¥\n",
    "    if data:\n",
    "        df = pd.DataFrame(data)\n",
    "        print(f\"\\nğŸ“Š ì¶”ì¶œëœ ë…¼ë¬¸ ìˆ˜: {len(df)}ê±´ ({START_YEAR}~í˜„ì¬)\")\n",
    "        df.to_csv(OUTPUT_FILE, index=False)\n",
    "        print(f\"âœ… ì €ì¥ ì™„ë£Œ: {OUTPUT_FILE} (ì´ íŒŒì¼ì€ í˜„ì¬ í´ë”ì— ìƒì„±ë¨)\")\n",
    "    else:\n",
    "        print(\"âš ï¸ ì¡°ê±´ì— ë§ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í•„í„°ë§ ì¡°ê±´ì„ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
    "\n",
    "# ì‹¤í–‰\n",
    "if __name__ == \"__main__\":\n",
    "    parse_arxiv_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1057dc7f-49c6-4133-8537-27f65b4483f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19085/2255113780.py:9: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(DATA_FILE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19085/2255113780.py\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# ëª¨ë¸ ë¡œë“œ (ê²€ìƒ‰ìš©)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all-MiniLM-L6-v2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "DATA_FILE = 'arxiv_cs_recent_filtered.csv'\n",
    "EMBEDDING_FILE = 'sbert_embeddings_sampled_all-MiniLM-L6-v2.npy' \n",
    "\n",
    "print(\"ğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘...\")\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "embeddings = np.load(EMBEDDING_FILE)\n",
    "print(\"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ (ê²€ìƒ‰ìš©)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to('cuda')\n",
    "\n",
    "def search_similar_papers(query_text, top_k=5):\n",
    "    # Query ì¸ì½”ë”©\n",
    "    query_vec = model.encode([query_text], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    \n",
    "    # ìœ ì‚¬ë„ ê³„ì‚° (Dot Product)\n",
    "    scores = np.dot(query_vec, embeddings.T).flatten()\n",
    "    \n",
    "    # ìƒìœ„ Kê°œ ì¶”ì¶œ\n",
    "    top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "    \n",
    "    print(f\"\\nğŸ” Query: {query_text[:100]}...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for rank, idx in enumerate(top_indices):\n",
    "        paper = df.iloc[idx]\n",
    "        score = scores[idx]\n",
    "        # ê²°ê³¼ ì¶œë ¥\n",
    "        print(f\"[{rank+1}] ìœ ì‚¬ë„: {score:.4f} | {paper['year']} | {paper['categories']}\")\n",
    "        print(f\"Title: {paper['title']}\")\n",
    "        print(f\"Link: https://arxiv.org/abs/{paper['id']}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "my_abstract = \"\"\"\n",
    "We propose a novel method for split federated learning to address data heterogeneity.\n",
    "Our approach uses gradient shuffling to improve convergence speed on non-IID data.\n",
    "\"\"\"\n",
    "search_similar_papers(my_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d234d805-49d4-42f4-930b-6753e4d73ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "DATA_FILE = 'arxiv_cs_recent_filtered.csv' \n",
    "# DATA_FILE = 'arxiv_cs_recent_filtered.csv' \n",
    "\n",
    "def extract_distinctive_keywords():\n",
    "    print(f\"ğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘... ({DATA_FILE})\")\n",
    "    try:\n",
    "        df = pd.read_csv(DATA_FILE)\n",
    "    except FileNotFoundError:\n",
    "        print(\"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
    "        return\n",
    "\n",
    "    # ë¶ˆìš©ì–´(ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´) ì¶”ê°€ ì œê±°\n",
    "    custom_stopwords = list(ENGLISH_STOP_WORDS) + [\n",
    "        'propose', 'method', 'paper', 'based', 'approach', 'results', 'model', \n",
    "        'performance', 'state', 'art', 'using', 'novel', 'learning', 'network',\n",
    "        'algorithm', 'models', 'proposed', 'data', 'experiments', 'show', 'task',\n",
    "        'neural', 'networks', 'deep', 'analysis', 'applications', 'study', 'use'\n",
    "    ]\n",
    "    \n",
    "    years = sorted(df['year'].unique())\n",
    "    keywords_by_year = {} # ì—°ë„ë³„ í‚¤ì›Œë“œ ì €ì¥ì†Œ\n",
    "    \n",
    "    print(\"ğŸ“Š ì—°ë„ë³„ í‚¤ì›Œë“œ ë¶„ì„ ë° ê³µí†µ ë‹¨ì–´ í•„í„°ë§ ì¤‘...\")\n",
    "    \n",
    "    # 1. ê° ì—°ë„ë³„ Top 20 ì¶”ì¶œí•˜ì—¬ ì €ì¥\n",
    "    for year in years:\n",
    "        year_docs = df[df['year'] == year]['title']\n",
    "        combined_text = \" \".join(year_docs.fillna(\"\"))\n",
    "        \n",
    "        # ë¹ˆë„ ë¶„ì„\n",
    "        vectorizer = CountVectorizer(stop_words=custom_stopwords, max_features=1000, ngram_range=(1, 2))\n",
    "        try:\n",
    "            X = vectorizer.fit_transform([combined_text])\n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "            \n",
    "            # ë¹ˆë„ìˆœ ì •ë ¬ ë° Top 20 ì¶”ì¶œ\n",
    "            row = X[0].toarray().flatten()\n",
    "            top_indices = row.argsort()[::-1][:50] \n",
    "            top_keywords = [feature_names[idx] for idx in top_indices]\n",
    "            \n",
    "            keywords_by_year[year] = top_keywords\n",
    "        except ValueError:\n",
    "            # ë°ì´í„°ê°€ ë„ˆë¬´ ì ì–´ì„œ ì–´íœ˜ê°€ ìƒì„± ì•ˆ ëœ ê²½ìš°\n",
    "            keywords_by_year[year] = []\n",
    "\n",
    "    # 2. ëª¨ë“  ì—°ë„ì— ê³µí†µì ìœ¼ë¡œ ë“±ì¥í•œ í‚¤ì›Œë“œ(êµì§‘í•©) ì°¾ê¸°\n",
    "    if keywords_by_year:\n",
    "        # ì²« ë²ˆì§¸ í•´ì˜ í‚¤ì›Œë“œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‹œì‘í•´ì„œ ê³„ì† êµì§‘í•©(intersection)ì„ êµ¬í•¨\n",
    "        common_keywords = set(keywords_by_year[years[0]])\n",
    "        for year in years[1:]:\n",
    "            common_keywords = common_keywords.intersection(set(keywords_by_year[year]))\n",
    "            \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"ğŸš« ì œì™¸ëœ ê³µí†µ í‚¤ì›Œë“œ (ëª¨ë“  ì—°ë„ ë“±ì¥): {', '.join(common_keywords)}\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "        # 3. ê³µí†µ í‚¤ì›Œë“œ ì œê±° í›„ ì¶œë ¥\n",
    "        print(f\"{'Year':<6} | {'Distinctive Trending Keywords (Unique per Year)':<60}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for year in years:\n",
    "            # ì›ë˜ ë¦¬ìŠ¤íŠ¸ ìˆœì„œ(ë¹ˆë„ìˆœ)ë¥¼ ìœ ì§€í•˜ë©´ì„œ ê³µí†µ í‚¤ì›Œë“œë§Œ ì œê±°\n",
    "            unique_list = [k for k in keywords_by_year[year] if k not in common_keywords]\n",
    "            \n",
    "            # ë„ˆë¬´ ë§ì´ ë¹ ì ¸ì„œ ë¦¬ìŠ¤íŠ¸ê°€ ì§§ì•„ì§ˆ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìƒìœ„ 10~15ê°œ ì •ë„ë§Œ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "            unique_str = ', '.join(unique_list[:15]) \n",
    "            print(f\"{year:<6} | {unique_str}\")\n",
    "            \n",
    "        print(\"=\"*80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extract_distinctive_keywords()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
